# Automating Design of Experiments using SBATCH and Python

The goal of this repository is to show a simplistic way of exploiting the embarrassingly parallel nature of a Slurm cluster to run a computational design of experiments in parallel. Such a DoE might show up during the discovery phase of new software, when the inevitable question arises: "what collection of parameter values work best for this application?" Or it might show up during an engineering simulation task to optimize parameters. Or it might show up in hyperparameter tuning of machine learning models. And so on...

Suppose for the sake of simplicity that our project requires us to run some black box software which accepts a known list of arbitrarily many space-separated parameters without flags, like the enclosed `black_box.sh`.

What we want is a program that runs `black_box.sh` but with a standardized interface. That interface will be accepting a file containing all possible  combinations of parameters, or treatments, we wish to process. One treatment will be written per line, which sounds like a `csv` file. We also need an integer pointing to the current row of interest for each individual run of `black_box.sh`. The standardized interface looks something like `payload.py`, which is a very simple wrapper around `black_box.sh` that achieves this using Python's `subprocess` module.

Now all that remains is to create an `sbatch` script that runs `payload.py` as an array job, and a `csv` file with all the treatments. Part of the `sbatch` script is a payload that actually does the heavy lifting of loading modules and calling `python` on `payload.py`. This is contained in `payload.sh`. The `sbatch` script and `csv` file are generated by functions in `sbatch.py`.

The function `generate_treatments()` takes in a `json` file and an output file path. The `json` file looks like `doe.json` and has a collection of named lists of parameters. These are transformed into a `csv` file using Python's builtin `itertools` library and Pandas. The key idea here is that `itertools` has a function `product()` which creates every Cartesian product by pulling one item at a time from all of the input lists. So if we have one list for `learning_rate` and one list for `batch_size`, it will create every possible list of the form `[learning_rate, batch_size]`. The fact that we can encapsulate this idea in a single function may seem anticlimactic, but frankly that's just Python. It has a very rich and expressive collection of builtin libraries.

The function `generate_array_job()` takes in a payload_file, i.e. `payload.sh`, a treatment count, i.e. from `generate_treatments()`, and an output file path for the shell script. The shell script is then generated in a straightforward way. For simplicity of illustration the `sbatch` parameters have been hardcoded in `sbatch.py`, to better show the familiar structure of a manually written `sbatch` file. There is an alternate version of the file in the folder `parameterized` which attempts to alleviate this issue using another `json` file containing the lists of parameters.

Running `sbatch.py` at the command line will produce example outputs from the default inputs outlined above, i.e. `doe.json` and `payload.sh`. The resulting files will be `doe.csv` and `run.sh`. Simply call `sbatch run.sh` on the cluster to submit the entire array job.

## Pushing Further...

Of course this is a simplistic way of approaching a complex problem: optimization in a large state space. There are more robust and faster methods using more advanced algorithms. Worse, there is no one-size-fits-all solution that can be recommended that will work for all optimization problems. The best that can be done is to point you to resources you might find helpful.

If your problem space is optimization in a large state space then...

- Blackbox optimization using multi-armed bandit Bayesian optimizer: https://github.com/ARM-software/mango
  - Useful if the state space is large or the shape of the response surface is completely unknown.
- Genetic optimization: https://pypi.org/project/geneticalgorithm/
  - Useful if parameters are interrelated such that combining good results is likely to produce a better result.
- Simulated annealing: https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.optimize.anneal.html
  - Useful if the parameters are not necessarily interrelated or there are many local minima. Especially useful if the state space has many dimensions.
- Gradient-based: https://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html#newton-conjugate-gradient-algorithm-method-newton-cg
  - Useful if the response surface is convex.
- Stochastic gradient-based: https://scikit-learn.org/stable/modules/sgd.html
  - Useful it the response surface is nearly convex but has many local minima.
